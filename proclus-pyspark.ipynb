{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import arffreader as ar\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "'''\n",
    "def findDimensions1(X, y, k, l, L, Mcurr):\n",
    "\tN, d = y.shape\n",
    "\tDis = [] # dimensions picked for the clusters\n",
    "\n",
    "\tZis = [] # Z for the remaining dimensions\n",
    "\tRem = [] # remaining dimensions\n",
    "\tMselidx = [] # id of the medoid indexing the dimensions in Zis and Rem\n",
    "\n",
    "\tfor i in range(len(Mcurr)):\n",
    "\t\tmi = Mcurr[i]\n",
    "\t\tprint(L[i])# Xij is the average distance from the points in L_i to m_i\n",
    "\t\ta = X.collect()[mi]\n",
    "\t\t#Xij = X.groupByKey().map(lambda row: np.abs(row[L[i]] - row[mi]).sum(axis = 0) / len(L[i]))\n",
    "\t\tXi = np.zeros(len(a))\n",
    "\t\t\n",
    "\t\tfor j in range(len(L[i])):\n",
    "\t\t\tb = L[i][j] \n",
    "\t\t\tc = X.collect()[b]\n",
    "\t\t\tx = np.asarray(c)\n",
    "\t\t\ty = np.asarray(a)\n",
    "\t\t\tXg = np.abs(x - y).sum(axis = 0)\n",
    "\t\t\t#Xi = np.add(Xg, Xi) #print(Xi) \n",
    "\t\tXij = Xg / len(L[i])\n",
    "\t\t#Xij = X.map(lambda X: np.abs(X.features - a)) #print(X[L[i]])# Xij here is an array, containing the avg dists in each dimension\n",
    "\t\t#print(Xij.take(1).foreach(println)) #Xij = np.abs(X[L[i]] - X[mi]).sum(axis = 0) / len(L[i])\n",
    "\t\tprint(\"XIJ IS :\",Xij)#print(Xij)\n",
    "\t\tYi = Xij.sum() / d # average distance over all dimensions\n",
    "\t\tDi = [] # relevant dimensions for m_i\n",
    "\t\tsi = np.sqrt(((Xij - Yi)**2).sum() / (d-1)) # standard deviations\n",
    "\t\tZij = (Xij - Yi) / si # z-scores of distances\n",
    "\t\tprint(Zij)\n",
    "\t\t# pick the smallest two:\n",
    "\t\to = np.argsort(Zij)\n",
    "\t\t#o = o[0]\n",
    "\t\tprint(o)\n",
    "\t\tDi.append(o[0])\n",
    "\t\tDi.append(o[1])\n",
    "\t\tDis.append(Di)\n",
    "\n",
    "\t\tfor j in range(2,d):\n",
    "\t\t\tZis.append(Zij[o[j]])\n",
    "\t\t\tRem.append(o[j])\n",
    "\t\t\tMselidx.append(i)\n",
    "\n",
    "\tif l != 2:\n",
    "\t\t# we need to pick the remaining dimensions\n",
    "\n",
    "\t\to = np.argsort(Zis)\n",
    "\t\t\n",
    "\t\tnremaining = k * l - k * 2\n",
    "\t\t# print \"still need to pick %d dimensions.\" % nremaining\n",
    "\n",
    "\t\t# we pick the remaining dimensions using a greedy strategy:\n",
    "\t\tj = 0\n",
    "\t\twhile nremaining > 0:\n",
    "\t\t\tmidx = Mselidx[o[j]]\n",
    "\t\t\tDis[midx].append(Rem[o[j]])\n",
    "\t\t\tj += 1\n",
    "\t\t\tnremaining -= 1\n",
    "\n",
    "\t#print \"selected:\"\n",
    "\t#print Dis\n",
    "\n",
    "\treturn Dis\n",
    "\t\t\n",
    "'''\n",
    "def manhattanSegmentalDist(x, y, Ds):\n",
    "\t\"\"\" Compute the Manhattan Segmental Distance between x and y considering\n",
    "\t\tthe dimensions on Ds.\"\"\"\n",
    "\tdist = 0\n",
    "\tfor d in Ds:\n",
    "\t\tdist += np.abs(x[d] - y[d])\n",
    "\treturn dist / len(Ds)\n",
    "\n",
    "def assignPoints(X, y, Mcurr, Dis):\n",
    "\n",
    "\tassigns = np.ones(y.shape[0]) * -1\n",
    "\t#X = X\n",
    "\t#for i in range(y.shape[0]):\n",
    "\tminDist = np.inf\n",
    "\tbest = -1\n",
    "\tfor j in range(len(Mcurr)):\n",
    "\t\tschema = sqlContext.createDataFrame(X)#b = X.collect()[i]\n",
    "\t\tschema.registerTempTable(\"data\")#a = X.collect()[Mcurr[j]]#dist = X.zipWithIndex().filter(lambda (key,index) : index = i).map(lambda (key,index) : key).collect()\n",
    "\t\tfeats = sqlContext.sql(\"SELECT features FROM data\")#print(X.first(),type(X[0],Dis[j])) \n",
    "\t\ta = feats.collect()[Mcurr[j]]\n",
    "\t\tprint(type(feats))        \n",
    "\t\tdist = feats.map(lambda x: manhattanSegmentalDist(x.features, a.features, Dis[j])).collect()#X.filter(lambda x: )#manhattanSegmentalDist(b.features, a.features, Dis[j])\n",
    "\t\t#dist = dis.first()#dist = dist.astype(np.int32)\n",
    "\t\tprint(\"dis is:\",dist,\"itertion\",j)\n",
    "\t\t#dist = manhattanSegmentalDist(X[i], X[Mcurr[j]], Dis[j])\n",
    "\t\t#if dist < minDist:\n",
    "\t#\t\tminDist = dist\n",
    "\t#\t\tbest = Mcurr[j]\n",
    "\n",
    "\t\tassigns = best\n",
    "\t#print(assigns)\n",
    "\treturn assigns\n",
    "\n",
    "\n",
    "def evaluateClusters(X, assigns, Dis, Mcurr):\n",
    "\n",
    "\tupperSum = 0.0\n",
    "\n",
    "\tfor i in range(len(Mcurr)):\t\t\n",
    "\t\tC = X[np.where(assigns == Mcurr[i])[0]] # points in cluster M_i\n",
    "\t\tCm = C.sum(axis = 0) / C.shape[0] # cluster centroid\n",
    "\t\tYsum = 0.0\n",
    "\n",
    "\t\tfor d in Dis[i]:\n",
    "\t\t\t# avg dist to centroid along dim d:\n",
    "\t\t\tYsum += np.sum(np.abs(C[:,d] - Cm[d])) / C.shape[0]\n",
    "\t\twi = Ysum / len(Dis[i])\n",
    "\n",
    "\t\tupperSum += C.shape[0] * wi\n",
    "\n",
    "\treturn upperSum / X.shape[0]\n",
    "\n",
    "def computeBadMedoids(X, assigns, Dis, Mcurr, minDeviation):\n",
    "\tN, d = X.shape\n",
    "\tk = len(Mcurr)\n",
    "\tMbad = []\n",
    "\tcounts = [len(np.where(assigns == i)[0]) for i in Mcurr]\n",
    "\tcte = int(np.ceil((N / k) * minDeviation))\n",
    "\n",
    "\t# get the medoid with least points:\n",
    "\tMbad.append(Mcurr[np.argsort(counts)[0]])\n",
    "\n",
    "\tfor i in range(len(counts)):\n",
    "\t\tif counts[i] < cte and Mcurr[i] not in Mbad:\n",
    "\t\t\tMbad.append(Mcurr[i])\n",
    "\n",
    "\treturn Mbad\n",
    "\n",
    "def proclus(X, y, k = 2, l = 3, minDeviation = 0.1, A = 30, B = 3, niters = 30, seed = 1234):\n",
    "\t\"\"\" Run PROCLUS on a database to obtain a set of clusters and \n",
    "\t\tdimensions associated with each one.\n",
    "\t\tParameters:\n",
    "\t\t----------\n",
    "\t\t- X: \t   \t\tthe data set\n",
    "\t\t- k: \t   \t\tthe desired number of clusters\n",
    "\t\t- l:\t   \t\taverage number of dimensions per cluster\n",
    "\t\t- minDeviation: for selection of bad medoids\n",
    "\t\t- A: \t   \t\tconstant for initial set of medoids\n",
    "\t\t- B: \t   \t\ta smaller constant than A for the final set of medoids\n",
    "\t\t- niters:  \t\tmaximum number of iterations for the second phase\n",
    "\t\t- seed:    \t\tseed for the RNG\n",
    "\t\"\"\"\n",
    "\tnp.random.seed(seed)\n",
    "\n",
    "\tN, d = y.shape\n",
    "\n",
    "\tif B > A:\n",
    "\t\traise Exception(\"B has to be smaller than A.\")\n",
    "\n",
    "\tif l < 2:\n",
    "\t\traise Exception(\"l must be >=2.\")\n",
    "\n",
    "\t###############################\n",
    "\t# 1.) Initialization phase\n",
    "\t###############################\n",
    "\n",
    "\t# first find a superset of the set of k medoids by random sampling\n",
    "\tidxs = np.arange(N)\n",
    "\tnp.random.shuffle(idxs)\n",
    "\tS = idxs[0:(A*k)]\n",
    "\tM = greedy2(X, S, B * k)\n",
    "\tprint(\"medoids are \",M)\n",
    "\t###############################\n",
    "\t# 2.) Iterative phase\n",
    "\t###############################\n",
    "\n",
    "\tBestObjective = np.inf\n",
    "\n",
    "\t# choose a random set of k medoids from M:\n",
    "\tMcurr = np.random.permutation(M)[0:k] # M current\n",
    "\tMbest = None # Best set of medoids found\n",
    "\n",
    "\tD = squareform(pdist(y)) # precompute the euclidean distance matrix\n",
    "\t#print(D)\n",
    "\tit = 0 # iteration counter\n",
    "\tL = [] # locality sets of the medoids, i.e., points within delta_i of m_i.\n",
    "\tDis = [] # important dimensions for each cluster\n",
    "\tassigns = [] # cluster membership assignments\n",
    "\n",
    "\twhile True:\n",
    "\t\tit += 1\n",
    "\t\tL = []#print(it)\n",
    "\n",
    "\t\tfor i in range(len(Mcurr)):\n",
    "\t\t\tmi = Mcurr[i]\n",
    "\t\t\t# compute delta_i, the distance to the nearest medoid of m_i:\n",
    "\t\t\tdi = D[mi,np.setdiff1d(Mcurr, mi)].min()\n",
    "\t\t\t#print(di)# compute L_i, points in sphere centered at m_i with radius d_i\n",
    "\t\t\tL.append(np.where(D[mi] <= di)[0])\n",
    "\n",
    "\t\t#print(L)# find dimensions:\n",
    "\t\tDis = findDimensions2(X, y, k, l, L, Mcurr)\n",
    "\t\tprint(\"dimensions are:\",Dis)\n",
    "\t\t# form the clusters:\n",
    "\t\tassigns = assignPoints(X, y, Mcurr, Dis)\n",
    "\t\t\n",
    "\t\t# evaluate the clusters:\n",
    "\t\tObjectiveFunction = evaluateClusters(X, assigns, Dis, Mcurr)\n",
    "\t\tprint(ObjectiveFunction)\n",
    "\t\tbadM = [] # bad medoids\n",
    "\n",
    "\t\tMold = Mcurr.copy()\n",
    "\n",
    "\t\tif ObjectiveFunction < BestObjective:\n",
    "\t\t\tBestObjective = ObjectiveFunction\n",
    "\t\t\tMbest = Mcurr.copy()\n",
    "\t\t\t# compute the bad medoids in Mbest:\n",
    "\t\t\tbadM = computeBadMedoids(X, assigns, Dis, Mcurr, minDeviation)\n",
    "\t\t\tprint (\"bad medoids:\")\n",
    "\t\t\tprint (badM)\n",
    "\n",
    "\t\tif len(badM) > 0:\n",
    "\t\t\t# replace the bad medoids with random points from M:\n",
    "\t\t\tprint (\"old mcurr:\")\n",
    "\t\t\tprint (Mcurr)\n",
    "\t\t\tMavail = np.setdiff1d(M, Mbest)\n",
    "\t\t\tnewSel = np.random.choice(Mavail, size = len(badM), replace = False)\n",
    "\t\t\tMcurr = np.setdiff1d(Mbest, badM)\n",
    "\t\t\tMcurr = np.union1d(Mcurr, newSel)\n",
    "\t\t\tprint (\"new mcurr:\")\n",
    "\t\t\tprint (Mcurr)\n",
    "\n",
    "\t\tprint (\"finished iter: %d\" % it)\n",
    "\n",
    "\t\tif np.allclose(Mold, Mcurr) or it >= niters:\n",
    "\t\t\tbreak\n",
    "\n",
    "\tprint (\"finished iterative phase...\")\n",
    "\n",
    "\t###############################\n",
    "\t# 3.) Refinement phase\n",
    "\t###############################\n",
    "\n",
    "\t# compute a new L based on assignments:\n",
    "\tL = []\n",
    "\tfor i in range(len(Mcurr)):\n",
    "\t\tmi = Mcurr[i]\n",
    "\t\tL.append(np.where(assigns == mi)[0])\n",
    "\n",
    "\tDis = findDimensions(X, y, k, l, L, Mcurr)\n",
    "\tassigns = assignPoints(X, Mcurr, Dis)\n",
    "\n",
    "\t# handle outliers:\n",
    "\n",
    "\t# smallest Manhattan segmental distance of m_i to all (k-1)\n",
    "\t# other medoids with respect to D_i:\n",
    "\tdeltais = np.zeros(k)\n",
    "\tfor i in range(k):\n",
    "\t\tminDist = np.inf\n",
    "\t\tfor j in range(k):\n",
    "\t\t\tif j != i:\n",
    "\t\t\t\tdist = manhattanSegmentalDist(X[Mcurr[i]], X[Mcurr[j]], Dis[i])\n",
    "\t\t\t\tif dist < minDist:\n",
    "\t\t\t\t\tminDist = dist\n",
    "\t\tdeltais[i] = minDist\n",
    "\n",
    "\t# mark as outliers the points that are not within delta_i of any m_i:\n",
    "\tfor i in range(len(assigns)):\n",
    "\t\tclustered = False\n",
    "\t\tfor j in range(k):\n",
    "\t\t\td = manhattanSegmentalDist(X[Mcurr[j]], X[i], Dis[j])\n",
    "\t\t\tif d <= deltais[j]:\n",
    "\t\t\t\tclustered = True\n",
    "\t\t\t\tbreak\n",
    "\t\tif not clustered:\n",
    "\t\t\t\n",
    "\t\t\t#print \"marked an outlier\"\n",
    "\t\t\tassigns[i] = -1\n",
    "\n",
    "\treturn (Mcurr, Dis, assigns)\n",
    "\n",
    "def computeBasicAccuracy(pred, expect):\n",
    "\t\"\"\" Computes the clustering accuracy by assigning\n",
    "\t\ta class to each cluster based on majority\n",
    "\t\tvoting and then comparing with the expected\n",
    "\t\tclass. \"\"\"\n",
    "\n",
    "\tif len(pred) != len(expect):\n",
    "\t\traise Exception(\"pred and expect must have the same length.\")\n",
    "\n",
    "\tuclu = np.unique(pred)\n",
    "\n",
    "\tacc = 0.0\n",
    "\n",
    "\tfor cl in uclu:\n",
    "\t\tpoints = np.where(pred == cl)[0]\n",
    "\t\tpclasses = expect[points]\n",
    "\t\tuclass = np.unique(pclasses)\n",
    "\t\tcounts = [len(np.where(pclasses == u)[0]) for u in uclass]\n",
    "\t\tmcl = uclass[np.argmax(counts)]\n",
    "\t\tacc += np.sum(np.repeat(mcl, len(points)) == expect[points])\n",
    "\n",
    "\tacc /= len(pred)\n",
    "\n",
    "\treturn acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(x,y):   \n",
    "    return numpy.sqrt(numpy.sum((x-y)**2))\n",
    "\n",
    "def greedy2(X, S, k):\n",
    "\t# remember that k = B * k here...\n",
    "\n",
    "\tM = [np.random.permutation(S)[0]] # M = {m_1}, a random point in S\n",
    "\t# print M\n",
    "\n",
    "\tA = np.setdiff1d(S, M) # A = S \\ M\n",
    "\t#print(len(A))\n",
    "\tdists = np.zeros(len(A))\n",
    "\n",
    "\t#for i in range(len(A)):\n",
    "\ty = X.collect()[M[0]]\n",
    "\t#print(y.features)#dis = X.filter\n",
    "\tdist = X.filter(lambda x: any(x[9] == A[j] for j in range(len(A))))#.map(lambda x: np.linalg.norm(x.features-y.features))\n",
    "\t#print(dist.count())\n",
    "\tdists = dist.map(lambda x: np.linalg.norm(x.features-y.features)).collect()\n",
    "\tprint(dists)\n",
    "\t\n",
    "\t#dists[i] = np.linalg.norm(X[A[i]] - X[M[0]]) # euclidean distance\n",
    "\n",
    "\t# print dists\n",
    "\n",
    "\tfor i in range(1, k):\n",
    "\t\t# choose medoid m_i as the farthest from previous medoids\n",
    "\n",
    "\t\tmidx = np.argmax(dists)\n",
    "\t\tmi = A[midx]\n",
    "\t\t\n",
    "\t\tM.append(mi)\n",
    "\t\t\t\t\n",
    "\t\t# update the distances, so they reflect the dist to the closest medoid:\n",
    "\t\tfor j in range(len(A)):\n",
    "\t\t\ty = X.collect()[mi]\n",
    "\t\t\tb = X.collect()[A[j]]#filter(lambda x: x[9] == A[j])\n",
    "\t\t\t#print(b.features)#.map(lambda x: np.linalg.norm(X.features - y.features)).collect()\n",
    "\t\t\tdists[j] = min(dists[j], np.linalg.norm(b.features - y.features))\n",
    "\t\t\n",
    "\t\t# remove mi entries from A and dists:\n",
    "\t\tA = np.delete(A, midx)\n",
    "\t\tdists = np.delete(dists, midx)\n",
    "\n",
    "\treturn np.array(M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDimensions2(X, y, k, l, L, Mcurr):\n",
    "\tN, d = y.shape\n",
    "\tDis = [] # dimensions picked for the clusters\n",
    "\t#print(len(L[0]))\n",
    "\tZis = [] # Z for the remaining dimensions\n",
    "\tRem = [] # remaining dimensions\n",
    "\tMselidx = [] # id of the medoid indexing the dimensions in Zis and Rem\n",
    "\n",
    "\tfor i in range(len(Mcurr)):\n",
    "\t\tmi = Mcurr[i]\n",
    "\t\tm = X.collect()[mi]\n",
    "\t\tDavg = X.filter(lambda x: any(x[9] == L[i][j] for j in range(len(L[i]))))# Xij is the average distance from the points in L_i to m_i\n",
    "\t\t#print(Davg.count())\n",
    "\t\tXij = Davg.map(lambda x: np.abs(x.features - m.features)).reduce(lambda x,y: np.true_divide(np.add(x,y),len(L[i])))#.reduce() # Xij here is an array, containing the avg dists in each dimension\n",
    "\t\t#print(Xij)\n",
    "\t\t#Xij = np.abs(X[L[i]] - X[mi]).sum(axis = 0) / len(L[i])\n",
    "\t\tprint(\"XIJ IS :\",Xij)\n",
    "\t\tYi = Xij.sum() / d # average distance over all dimensions\n",
    "\t\tDi = [] # relevant dimensions for m_i\n",
    "\t\tsi = np.sqrt(((Xij - Yi)**2).sum() / (d-1)) # standard deviations\n",
    "\t\tZij = (Xij - Yi) / si # z-scores of distances\n",
    "\n",
    "\t\to = np.argsort(Zij)# pick the smallest two:\n",
    "\t\tprint(o)\n",
    "\t\tDi.append(o[0])\n",
    "\t\tDi.append(o[1])\n",
    "\t\tDis.append(Di)\n",
    "\n",
    "\t\tfor j in range(2,d):\n",
    "\t\t\tZis.append(Zij[o[j]])\n",
    "\t\t\tRem.append(o[j])\n",
    "\t\t\tMselidx.append(i)\n",
    "\n",
    "\tif l != 2:\n",
    "\t\t# we need to pick the remaining dimensions\n",
    "\n",
    "\t\to = np.argsort(Zis)\n",
    "\t\t\n",
    "\t\tnremaining = k * l - k * 2\n",
    "\t\t# print \"still need to pick %d dimensions.\" % nremaining\n",
    "\n",
    "\t\t# we pick the remaining dimensions using a greedy strategy:\n",
    "\t\tj = 0\n",
    "\t\twhile nremaining > 0:\n",
    "\t\t\tmidx = Mselidx[o[j]]\n",
    "\t\t\tDis[midx].append(Rem[o[j]])\n",
    "\t\t\tj += 1\n",
    "\t\t\tnremaining -= 1\n",
    "\n",
    "\t#print \"selected:\"\n",
    "\t#print Dis\n",
    "\n",
    "\treturn Dis\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000\n",
      "file is of type MapPartitionsRDD[390] at javaToPython at NativeMethodAccessorImpl.java:0\n",
      "Using seed 9028844\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 7181.0 failed 1 times, most recent failure: Lost task 1.0 in stage 7181.0 (TID 7194, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-a450cffeded5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproclus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m#print \"Accuracy: %.4f\" % computeBasicAccuracy(A, sup)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-f43bbae44ca8>\u001b[0m in \u001b[0;36mproclus\u001b[0;34m(X, y, k, l, minDeviation, A, B, niters, seed)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgreedy2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"medoids are \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;31m###############################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-c2994448beaa>\u001b[0m in \u001b[0;36mgreedy2\u001b[0;34m(X, S, k)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m#for i in range(len(A)):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;31m#print(y.features)#dis = X.filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.map(lambda x: np.linalg.norm(x.features-y.features))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 7181.0 failed 1 times, most recent failure: Lost task 1.0 in stage 7181.0 (TID 7194, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 40484)\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.5/socketserver.py\", line 313, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/anaconda3/lib/python3.5/socketserver.py\", line 341, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/anaconda3/lib/python3.5/socketserver.py\", line 354, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/anaconda3/lib/python3.5/socketserver.py\", line 681, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 268, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/lib/spark/python/pyspark/serializers.py\", line 714, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "#import adjrand\n",
    "#import arffreader as ar\n",
    "\n",
    "##### read data#####\n",
    "\"\"\"\n",
    "#X = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0]).mapPartitions(readPointBatch).cache()\n",
    "#y, sup = ar.readarff(\"/usr/lib/spark/examples/src/main/python/Major_proj/datasets/Archive/leukemia2.csv\") #read from arff\n",
    "#y = genfromtxt('/usr/lib/spark/examples/src/main/python/Major_proj/datasets/Archive/leukemia1.csv',delimiter=',',skip_header=1) #path to csv file\n",
    "y = pd.read_csv('/usr/lib/spark/examples/src/main/python/Major_proj/datasets/Archive/leukemia1.csv',delimiter=',')\n",
    "# change label values here\n",
    "#print(y)\n",
    "print(y.dtypes)\n",
    "#df =  df.withColumn(\"cls\", df[\"CLASS\"].getItem(0).cast(\"string\")) \n",
    "y = y.replace({'ALL': 0, 'AML': 1})\n",
    "sup = [item[-1] for item in y]\n",
    "sup = np.array(sup)\n",
    "sup = sup.ravel()\n",
    "print(y)\n",
    "\"\"\"\n",
    "sqlContext = SQLContext(sc)\n",
    "g = pd.read_csv('/usr/lib/spark/examples/src/main/python/Major_proj/datasets/Archive/diabetes.csv',delimiter=',')\n",
    "#print(y)\n",
    "y = g.iloc[:, :-1]\n",
    "#dftt = dft.toDF()\n",
    "#print(type(dftt))\n",
    "X = dft.rdd\n",
    "print(X.count())\n",
    "print(\"file is of type\",X)\n",
    "Dims = [0,1]\n",
    "#plotter.plotDataset(X, D = Dims) # plot 0-1 dimensions\n",
    "\n",
    "R = 1 # toggle run proclus\n",
    "RS = 0 # toggle use random seed\n",
    "\n",
    "if R: # run proclus\n",
    "\trseed = 9028844\n",
    "\tif RS:\n",
    "\t\trseed = np.random.randint(low = 0, high = 1239831)\n",
    "\n",
    "\tprint (\"Using seed %d\" % rseed)\n",
    "\tk = 5\n",
    "\tl = 3\n",
    "\tM, D, A = proclus(X, y, k, l, seed = rseed)\n",
    "\tprint(A)\n",
    "\t#print \"Accuracy: %.4f\" % computeBasicAccuracy(A, sup)\n",
    "\t#print \"Adjusted rand index: %.4f\" % adjrand.computeAdjustedRandIndex(A, sup)\n",
    "\n",
    "\t#plotter.plotClustering(X, M, A, D = Dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readPointBatch(iterator):\n",
    "    strs = list(iterator)\n",
    "    matrix = np.zeros((len(strs), D + 1))\n",
    "    for i, s in enumerate(strs):\n",
    "        matrix[i] = np.fromstring(s.replace(',', ' '), dtype=np.float32, sep=' ')\n",
    "    return [matrix]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:25: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('_c0', 'float'),\n",
       " ('_c1', 'float'),\n",
       " ('_c2', 'float'),\n",
       " ('_c3', 'float'),\n",
       " ('_c4', 'float'),\n",
       " ('_c5', 'float')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocessing\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "spark = SparkSession.builder.appName(\"proclus\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "#Load and parse the data\n",
    "#dataset = sqlContext.read.format('com.databricks.spark.csv').options(header='true').load('/usr/lib/spark/examples/src/main/python/Major_proj/datasets/Archive/glass.csv')\n",
    "#df = pd.read_csv('/usr/lib/spark/examples/src/main/python/Major_proj/datasets/Archive/glass.csv',delimiter=',') # path to file here\n",
    "#dataset = sqlContext.read.format('com.databricks.spark.csv').options(header='true').load('/usr/lib/spark/examples/src/main/python/Major_proj/diabetes.csv')\n",
    "df = pd.read_csv('/root/Downloads/3MillionG9cluster.csv',delimiter=',') # path to file here\n",
    "#dataset = spark.read.option(\"header\",\"true\").csv('/usr/lib/spark/examples/src/main/python/Major_proj/diabetes.csv')\n",
    "dataset = spark.read.csv('/root/Downloads/3MillionG9cluster.csv')\n",
    "\n",
    "#dataframes = map(lambda r: spark.read.text(r[0]), filenames)\n",
    "#ll_lines_df = reduce(lambda df1, df2: df1.unionAll(df2), dataframes)\n",
    "a = len(df.columns)\n",
    "dat = sc.textFile(\"/root/Downloads/3MillionG9cluster.csv\")\n",
    "label = df.as_matrix(columns=df.columns[a-1:])\n",
    "label = label.ravel()\n",
    "dataset = dataset.na.drop()\n",
    "#print(label)\n",
    "dataset.dtypes\n",
    "\n",
    "from pyspark.sql.types import FloatType\n",
    "dataset = dataset.withColumn(\"_c1\", dataset[\"_c1\"].cast(FloatType()))\n",
    "dataset = dataset.withColumn(\"_c2\", dataset[\"_c2\"].cast(FloatType()))\n",
    "dataset = dataset.withColumn(\"_c3\", dataset[\"_c3\"].cast(FloatType()))\n",
    "dataset = dataset.withColumn(\"_c4\", dataset[\"_c4\"].cast(FloatType()))\n",
    "dataset = dataset.withColumn(\"_c5\", dataset[\"_c5\"].cast(FloatType()))\n",
    "dataset = dataset.withColumn(\"_c0\", dataset[\"_c0\"].cast(FloatType()))\n",
    "\n",
    "'''\n",
    "dataset = dataset.withColumn(\"Glucose\", dataset[\"Glucose\"].cast(FloatType()))\n",
    "dataset = dataset.withColumn(\"BloodPressure\", dataset[\"BloodPressure\"].cast(FloatType()))\n",
    "dataset = dataset.withColumn(\"SkinThickness\", dataset[\"SkinThickness\"].cast(FloatType()))\n",
    "dataset = dataset.withColumn(\"Insulin\", dataset[\"Insulin\"].cast(FloatType()))\n",
    "dataset = dataset.withColumn(\"DiabetesPedigreeFunction\", dataset[\"DiabetesPedigreeFunction\"].cast(FloatType()))\n",
    "dataset = dataset.withColumn(\"BMI\", dataset[\"BMI\"].cast(FloatType()))\n",
    "dataset = dataset.withColumn(\"Age\", dataset[\"Age\"].cast(FloatType()))\n",
    "'''\n",
    "\n",
    "#dataset.cast(FloatType())\n",
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c1: float, _c2: float, _c3: float, _c4: float, _c5: float, features: vector]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocessing\n",
    "dft = dataset.select(*(dataset[c].cast(\"float\").alias(c) for c in dataset.columns[1:]))\n",
    "\n",
    "#from pyspark.sql.functions import monotonically_increasing_id \n",
    "\n",
    "#dft = dft.select(\"*\").withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "#split outcome\n",
    "#colm = (\"'class'\")\n",
    "#dft = dft.drop(colm)\n",
    "#print(dft.head())\n",
    "from pyspark.ml.feature import VectorAssembler # in spark api we have to convert data to vectors in order to run the model.\n",
    "\n",
    "# creating an instance of the vector assembler \n",
    "assembler = VectorAssembler(inputCols = dft.columns, outputCol = 'features')\n",
    "\n",
    "# transforming dataframe into vector assembler \n",
    "final_df = assembler.transform(dft)\n",
    "#final_df.drop('Outcome')\n",
    "#final_df = final_df.rdd.map(tuple)\n",
    "#df = final_df.ix[:,-1]\n",
    "#df = final_df.select(\"features\").rdd.map(lambda x: x).collect()\n",
    "#print(df.show())\n",
    "#df = final_df.select('features')\n",
    "#df = dfth[0].features.toArray()     # to array\n",
    "#df = df.rdd\n",
    "#dft = final_df.select(\"features\") \n",
    "dft = final_df\n",
    "dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy(X, S, k):\n",
    "\t# remember that k = B * k here...\n",
    "\n",
    "\tM = [np.random.permutation(S)[0]] # M = {m_1}, a random point in S\n",
    "\t# print M\n",
    "\n",
    "\tA = np.setdiff1d(S, M) # A = S \\ M\n",
    "\tdists = np.zeros(len(A))\n",
    "\ta = M[0]\n",
    "\tprint(X)#values = X.first().collect()\n",
    "\tmedi = X.collect()[a]\n",
    "\t#print(medi)\n",
    "\tfor i in range(len(A)):\n",
    "\t\tb = A[i]\n",
    "\t\titr = X.collect()[b]  \n",
    "\t\t#print(itr)\n",
    "\t\tx = np.asarray(medi)\n",
    "\t\ty = np.asarray(itr)\n",
    "\t\tdists[i] = np.linalg.norm(x[0] - y[0]) # euclidean distance\n",
    "\n",
    "\tprint (dists)\n",
    "\n",
    "\tfor i in range(1, k):\n",
    "\t\t# choose medoid m_i as the farthest from previous medoids\n",
    "\n",
    "\t\tmidx = np.argmax(dists)\n",
    "\t\tmi = A[midx]\n",
    "\t\t\n",
    "\t\tM.append(mi)\n",
    "\t\tmid = X.collect()[mi]\n",
    "\t\t# update the distances, so they reflect the dist to the closest medoid:\n",
    "\t\tfor j in range(len(A)):\n",
    "\t\t\tb = A[j]\n",
    "\t\t\titr_dist = X.collect()[b] \n",
    "\t\t\tx = np.asarray(mid)\n",
    "\t\t\ty = np.asarray(itr_dist)\n",
    "\t\t\tdists[j] = min(dists[j], np.linalg.norm(x[0] - y[0]))\n",
    "\t\t\n",
    "\t\t# remove mi entries from A and dists:\n",
    "\t\tA = np.delete(A, midx)\n",
    "\t\tdists = np.delete(dists, midx)\n",
    "\tprint(np.array(M))\n",
    "\treturn np.array(M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def findDimensions(X, y, k, l, L, Mcurr):\n",
    "\tN, d = y.shape\n",
    "\tDis = [] # dimensions picked for the clusters\n",
    "\n",
    "\tZis = [] # Z for the remaining dimensions\n",
    "\tRem = [] # remaining dimensions\n",
    "\tMselidx = [] # id of the medoid indexing the dimensions in Zis and Rem\n",
    "\n",
    "\tfor i in range(len(Mcurr)):\n",
    "\t\tmi = Mcurr[i]\n",
    "\t\t#a = X.collect()[mi]\n",
    "\t\t# Xij is the average distance from the points in L_i to m_i\n",
    "\t\t#X1 = X.zipWithIndex()\n",
    "\t\t#print(X1.first())\n",
    "\t\tXij = X.filter(lambda X : X[0]).map(absolute(X,L,mi)).collect()  # dimension\n",
    "\t\t#Xij = X.rdd.zipWithIndex().filter(lambda (key,index) : index == L[i]).map(lambda (key,index): np.abs(X[L[i]] - X[mi])/len(L[i])).collect()  # dimension\n",
    "\t\t#Xij = np.abs(X[L[i]] - X[mi]).sum(axis = 0) / len(L[i])\n",
    "\t\tprint(\"XIJ IS :\",Xij)\n",
    "\t\tYi = Xij.sum() / d # average distance over all dimensions\n",
    "\t\tDi = [] # relevant dimensions for m_i\n",
    "\t\tsi = np.sqrt(((Xij - Yi)**2).sum() / (d-1)) # standard deviations\n",
    "\t\tZij = (Xij - Yi) / si # z-scores of distances\n",
    "\n",
    "\t\to = np.argsort(Zij)# pick the smallest two:\n",
    "\t\tprint(o)\n",
    "\t\tDi.append(o[0])\n",
    "\t\tDi.append(o[1])\n",
    "\t\tDis.append(Di)\n",
    "\n",
    "\t\tfor j in range(2,d):\n",
    "\t\t\tZis.append(Zij[o[j]])\n",
    "\t\t\tRem.append(o[j])\n",
    "\t\t\tMselidx.append(i)\n",
    "\n",
    "\tif l != 2:\n",
    "\t\t# we need to pick the remaining dimensions\n",
    "\n",
    "\t\to = np.argsort(Zis)\n",
    "\t\t\n",
    "\t\tnremaining = k * l - k * 2\n",
    "\t\t# print \"still need to pick %d dimensions.\" % nremaining\n",
    "\n",
    "\t\t# we pick the remaining dimensions using a greedy strategy:\n",
    "\t\tj = 0\n",
    "\t\twhile nremaining > 0:\n",
    "\t\t\tmidx = Mselidx[o[j]]\n",
    "\t\t\tDis[midx].append(Rem[o[j]])\n",
    "\t\t\tj += 1\n",
    "\t\t\tnremaining -= 1\n",
    "\n",
    "\t#print \"selected:\"\n",
    "\t#print Dis\n",
    "\n",
    "\treturn Dis\n",
    "\t\t\n",
    "\n",
    "def absolute(X,L,mi):\n",
    "    g = []\n",
    "    for j in range(len(L)):\n",
    "        print(X.collect()[j])\n",
    "        g = np.abs(np.asarray(X.collect()[j]) - np.asarray(X.collect()[mi])).sum(axis = 0)/len(L)\n",
    "        print(g)\n",
    "        y.append(g)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id='0', Pregnancies='6', Glucose='148', BloodPressure='72', SkinThickness='35', Insulin='0', BMI='33.6', DiabetesPedigreeFunction='0.627', Age='50', Outcome='1')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat = dataset.rdd\n",
    "dat.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dat.filter(lambda x: x[0] == 54 or x[0] == 4)\n",
    "y.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
