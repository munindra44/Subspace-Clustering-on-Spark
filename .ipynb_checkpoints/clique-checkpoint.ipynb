{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse.csgraph\n",
    "\n",
    "from Cluster import Cluster\n",
    "from sklearn import metrics\n",
    "from ast import literal_eval\n",
    "\n",
    "from Visualization import plot_clusters\n",
    "\n",
    "\n",
    "# Inserts joined item into candidates list only if its dimensionality fits\n",
    "def insert_if_join_condition(candidates, item, item2, current_dim):\n",
    "    joined = []\n",
    "    for i in range(len(item)):\n",
    "        joined.append(item[i])\n",
    "    for i in range(len(item2)):\n",
    "        joined.append(item2[i])\n",
    "\n",
    "    # Count number of dimensions\n",
    "    dims = set()\n",
    "    for i in range(len(joined)):\n",
    "        dims.add(int(joined[i][0]))\n",
    "\n",
    "    # Insert if it fits\n",
    "    if len(dims) == current_dim:\n",
    "        candidates.append(joined)\n",
    "\n",
    "\n",
    "# Prune all candidates, which has a (k-1) dimensional projection not in (k-1) dim dense units\n",
    "def prune(candidates, prev_dim_dense_units):\n",
    "    for i in range(len(candidates)):\n",
    "        for j in range(len(candidates[i])):\n",
    "            if not prev_dim_dense_units.__contains__([candidates[i][j]]):\n",
    "                candidates.remove(candidates[i])\n",
    "                break\n",
    "\n",
    "\n",
    "def self_join(prev_dim_dense_units, dim):\n",
    "    candidates = []\n",
    "    for i in range(len(prev_dim_dense_units)):\n",
    "        for j in range(i + 1, len(prev_dim_dense_units)):\n",
    "            insert_if_join_condition(\n",
    "                candidates, prev_dim_dense_units[i], prev_dim_dense_units[j], dim)\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def is_data_in_projection(tuple, candidate, xsi):\n",
    "    for dim in candidate:\n",
    "        element = tuple[dim[0]]\n",
    "        if int(element * xsi % xsi) != dim[1]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_dense_units_for_dim(data, prev_dim_dense_units, dim, xsi, tau):\n",
    "    candidates = self_join(prev_dim_dense_units, dim)\n",
    "    prune(candidates, prev_dim_dense_units)\n",
    "\n",
    "    # Count number of elements in candidates\n",
    "    projection = np.zeros(len(candidates))\n",
    "    number_of_data_points = np.shape(data)[0]\n",
    "    for dataIndex in range(number_of_data_points):\n",
    "        for i in range(len(candidates)):\n",
    "            if is_data_in_projection(data[dataIndex], candidates[i], xsi):\n",
    "                projection[i] += 1\n",
    "    print(\"projection: \", projection)\n",
    "\n",
    "    # Return elements above density threshold\n",
    "    is_dense = projection > tau * number_of_data_points\n",
    "    print(\"is_dense: \", is_dense)\n",
    "    return np.array(candidates)[is_dense]\n",
    "\n",
    "\n",
    "def build_graph_from_dense_units(dense_units):\n",
    "    graph = np.identity(len(dense_units))\n",
    "    for i in range(len(dense_units)):\n",
    "        for j in range(len(dense_units)):\n",
    "            graph[i, j] = get_edge(dense_units[i], dense_units[j])\n",
    "    return graph\n",
    "\n",
    "\n",
    "def get_edge(node1, node2):\n",
    "    dim = len(node1)\n",
    "    distance = 0\n",
    "\n",
    "    for i in range(dim):\n",
    "        if node1[i][0] != node2[i][0]:\n",
    "            return 0\n",
    "        distance += abs(node1[i][1] - node2[i][1])\n",
    "        if distance > 1:\n",
    "            return 0\n",
    "    return 1\n",
    "\n",
    "\n",
    "def save_to_file(clusters, file_name):\n",
    "    file = open(os.path.join(os.path.abspath(os.path.dirname(\n",
    "        __file__)), file_name), encoding='utf-8', mode=\"w+\")\n",
    "    for i, c in enumerate(clusters):\n",
    "        c.id = i\n",
    "        file.write(\"Cluster \" + str(i) + \":\\n\" + str(c))\n",
    "    file.close()\n",
    "\n",
    "\n",
    "def get_cluster_data_point_ids(data, cluster_dense_units, xsi):\n",
    "    point_ids = set()\n",
    "\n",
    "    # Loop through all dense unit\n",
    "    for i in range(np.shape(cluster_dense_units)[0]):\n",
    "        tmp_ids = set(range(np.shape(data)[0]))\n",
    "        # Loop through all dimensions of dense unit\n",
    "        for j in range(np.shape(cluster_dense_units)[1]):\n",
    "            feature_index = cluster_dense_units[i][j][0]\n",
    "            range_index = cluster_dense_units[i][j][1]\n",
    "            tmp_ids = tmp_ids & set(\n",
    "                np.where(np.floor(data[:, feature_index] * xsi % xsi) == range_index)[0])\n",
    "        point_ids = point_ids | tmp_ids\n",
    "\n",
    "    return point_ids\n",
    "\n",
    "\n",
    "def get_clusters(dense_units, data, xsi):\n",
    "    graph = build_graph_from_dense_units(dense_units)\n",
    "    number_of_components, component_list = scipy.sparse.csgraph.connected_components(\n",
    "        graph, directed=False)\n",
    "\n",
    "    dense_units = np.array(dense_units)\n",
    "    clusters = []\n",
    "    # For every cluster\n",
    "    for i in range(number_of_components):\n",
    "        # Get dense units of the cluster\n",
    "        cluster_dense_units = dense_units[np.where(component_list == i)]\n",
    "        print(\"cluster_dense_units: \", cluster_dense_units.tolist())\n",
    "\n",
    "        # Get dimensions of the cluster\n",
    "        dimensions = set()\n",
    "        for j in range(len(cluster_dense_units)):\n",
    "            for k in range(len(cluster_dense_units[j])):\n",
    "                dimensions.add(cluster_dense_units[j][k][0])\n",
    "\n",
    "        # Get points of the cluster\n",
    "        cluster_data_point_ids = get_cluster_data_point_ids(\n",
    "            data, cluster_dense_units, xsi)\n",
    "        # Add cluster to list\n",
    "        clusters.append(Cluster(cluster_dense_units,\n",
    "                                dimensions, cluster_data_point_ids))\n",
    "    #print(clusters)\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def get_one_dim_dense_units(data, tau, xsi):\n",
    "    number_of_data_points = np.shape(data)[0]\n",
    "    number_of_features = np.shape(data)[1]\n",
    "    projection = np.zeros((xsi, number_of_features))\n",
    "    for f in range(number_of_features):\n",
    "        for element in data[:, f]:\n",
    "            projection[int(element * xsi % xsi), f] += 1\n",
    "    print(\"1D projection:\\n\", projection, \"\\n\")\n",
    "    is_dense = projection > tau * number_of_data_points\n",
    "    print(\"is_dense:\\n\", is_dense)\n",
    "    one_dim_dense_units = []\n",
    "    for f in range(number_of_features):\n",
    "        for unit in range(xsi):\n",
    "            if is_dense[unit, f]:\n",
    "                one_dim_dense_units.append([[f, unit]])\n",
    "    return one_dim_dense_units\n",
    "\n",
    "\n",
    "# Normalize data in all features (1e-5 padding is added because clustering works on [0,1) interval)\n",
    "def normalize_features(data):\n",
    "    normalized_data = data\n",
    "    number_of_features = np.shape(normalized_data)[1]\n",
    "    for f in range(number_of_features):\n",
    "        normalized_data[:, f] -= min(normalized_data[:, f]) - 1e-5\n",
    "        normalized_data[:, f] *= 1 / (max(normalized_data[:, f]) + 1e-5)\n",
    "    return normalized_data\n",
    "\n",
    "\n",
    "def evaluate_clustering_performance(df, clusters, labels):\n",
    "    set_of_dimensionality = set()\n",
    "    for cluster in clusters:\n",
    "        set_of_dimensionality.add(frozenset(cluster.dimensions))\n",
    "    max_f1 =[]\n",
    "    max_p = []\n",
    "    # Evaluating performance in all dimensionality\n",
    "    for dim in set_of_dimensionality:\n",
    "        print(\"\\nEvaluating clusters in dimension: \", list(dim))\n",
    "        # Finding clusters with same dimensions\n",
    "        clusters_in_dim = []\n",
    "        for c in clusters:\n",
    "            if c.dimensions == dim:\n",
    "                clusters_in_dim.append(c)\n",
    "        clustering_labels = np.zeros(np.shape(labels))\n",
    "        for i, c in enumerate(clusters_in_dim):\n",
    "            clustering_labels[list(c.data_point_ids)] = i + 1\n",
    "            clustering_labels1 = clustering_labels.astype(int)\n",
    "            #clustering_labels1 = clustering_labels1.tolist()\n",
    "            #print(\"clusterssssss\", clustering_labels1)\n",
    "        print(\"Number of clusters: \", len(clusters_in_dim))\n",
    "        \n",
    "        #print(dim)\n",
    "        #if (list(dim) == [40]):\n",
    "        #    print(\"label \",labels)\n",
    "        #    print(\"predicts\",clustering_labels1)\n",
    "        orig_l = list(set(clustering_labels1))\n",
    "        #print(orig_l)\n",
    "        d = len(list(set(clustering_labels1)))\n",
    "        #print(\"predicts labels size: \",d)\n",
    "        pred_l = (np.arange(d)).tolist()\n",
    "        for i in range(d):\n",
    "            #print(orig_l[i])\n",
    "            #print(pred_l[i])\n",
    "            clustering_labels1[clustering_labels1 == orig_l[i]] = pred_l[i]\n",
    "        #print(pred_l)\n",
    "        #print(\"clusterssssss\", clustering_labels1)\n",
    "        best_k = d\n",
    "        #print(best_k)\n",
    "        maxi,clus = max_ele(labels,clustering_labels1,best_k) \n",
    "        print(\"max index is \",maxi)\n",
    "        print(\"max nos in ach aray\",clus)\n",
    "        precision,recall,f1_score = metrics(maxi,labels,clus,best_k) # get metrics\n",
    "        \n",
    "        f1_value = np.average(f1_score)\n",
    "\n",
    "        print(\"F1-Value of the clusters is: \",f1_value)\n",
    "        purity = purity_fn(df,clus,maxi,best_k)\n",
    "        \n",
    "        max_f1.append(f1_value)\n",
    "        print(max_f1)\n",
    "        max_p.append(purity)\n",
    "        \n",
    "    return max_f1,max_p\n",
    "        #print(\"Number of clusters: \", len(clusters_in_dim))\n",
    "        #print(\"Adjusted Rand index: \", metrics.adjusted_rand_score(\n",
    "        #    labels, clustering_labels))\n",
    "        #print(\"Mutual Information: \", metrics.adjusted_mutual_info_score(\n",
    "        #    labels, clustering_labels))\n",
    "        \n",
    "        #print(\"Homogeneity, completeness, V-measure: \",\n",
    "        #      metrics.homogeneity_completeness_v_measure(labels, clustering_labels))\n",
    "\n",
    "        #print(\"Fowlkes-Mallows: \",\n",
    "        #      metrics.fowlkes_mallows_score(labels, clustering_labels))\n",
    "\n",
    "\n",
    "def run_clique(data, xsi, tau):\n",
    "    # Finding 1 dimensional dense units\n",
    "    dense_units = get_one_dim_dense_units(data, tau, xsi)\n",
    "\n",
    "    # Getting 1 dimensional clusters\n",
    "    clusters = get_clusters(dense_units, data, xsi)\n",
    "\n",
    "    # Finding dense units and clusters for dimension > 2\n",
    "    current_dim = 2\n",
    "    number_of_features = np.shape(data)[1]\n",
    "    while (current_dim <= number_of_features) & (len(dense_units) > 0):\n",
    "        print(\"\\n\", str(current_dim), \" dimensional clusters:\")\n",
    "        dense_units = get_dense_units_for_dim(\n",
    "            data, dense_units, current_dim, xsi, tau)\n",
    "        for cluster in get_clusters(dense_units, data, xsi):\n",
    "            clusters.append(cluster)\n",
    "        current_dim += 1\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def read_labels(delimiter, label_column, path):\n",
    "    return np.genfromtxt(path, dtype=\"U10\", delimiter=delimiter, usecols=[label_column])\n",
    "\n",
    "\n",
    "def read_data(delimiter, feature_columns, path):\n",
    "    return np.genfromtxt(path, dtype=float, delimiter=delimiter, usecols=feature_columns)\n",
    "\n",
    "import arffreader as ar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def max_ele(label,predicts,k):\n",
    "    a = Counter(label) #label's count\n",
    "    s = len(predicts)\n",
    "    \n",
    "    x = len(a)\n",
    "    #print(x)\n",
    "    ocr = []\n",
    "    for i in range(x):\n",
    "        temp = []\n",
    "        for j in range(s):\n",
    "            if label[j] == i:\n",
    "                temp.append(predicts[j])\n",
    "        ocr.append(temp)\n",
    "            \n",
    "    clus = []\n",
    "    for i in range(x):\n",
    "        y =  Counter(ocr[i])\n",
    "        #print(y)\n",
    "        clus_ocr = []\n",
    "        for j in range(k):\n",
    "            clus_ocr.append(y[j])\n",
    "        \n",
    "        clus.append(clus_ocr)\n",
    "    #print(clus)\n",
    "    \n",
    "    maxi = []\n",
    "    maxi_count =[]\n",
    "    idx = 0\n",
    "    for j in range(k):\n",
    "        ma = 0 \n",
    "        for i in range(x):\n",
    "            if (ma < clus[i][j]):\n",
    "                idx = i\n",
    "            ma = max(clus[i][j],ma)\n",
    "        maxi.append(idx)\n",
    "        maxi_count.append(ma)\n",
    "    #print(maxi)\n",
    "    return maxi,clus\n",
    "\n",
    "\n",
    "def metrics(maxi,label,clus,k):\n",
    "    prec = []\n",
    "    rec = []\n",
    "    f1 = []\n",
    "    ct = Counter(label)\n",
    "    #print(clus)\n",
    "    for j in range(k):\n",
    "        x = (clus[maxi[j]][j])\n",
    "        y = (sum([item[j] for item in clus]))\n",
    "        z = float(x)/float(y)\n",
    "        #print(z)\n",
    "        prec.append(z)\n",
    "        rc = (clus[maxi[j]][j])\n",
    "        rec.append(float(rc)/ct[maxi[j]])\n",
    "        f = (2*prec[j]*rec[j])/(prec[j]+rec[j])\n",
    "        f1.append(f)\n",
    "\n",
    "    print(\"precision : \",prec)\n",
    "    print(\"recall are: \",rec)\n",
    "    print(\"f1-score is: \",f1)\n",
    "    return prec,rec,f1\n",
    "\n",
    "def purity_fn(df,clus,maxi,k):\n",
    "    shape = df.shape\n",
    "    r_len = shape[0]\n",
    "    num = 0\n",
    "    for i in range(k):\n",
    "        num = num + float(clus[maxi[i]][i])\n",
    "    purity = num/r_len\n",
    "    print(\"Purity is: \",purity)\n",
    "    return purity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.46      0.7       0.67     ... -0.04      0.16           nan]\n",
      " [ 0.02      0.59      0.45     ... -0.14     -1.15           nan]\n",
      " [-0.32     -0.63     -0.46     ...  0.29      0.25           nan]\n",
      " ...\n",
      " [ 0.09     -0.078375  0.85     ...  0.09     -0.53           nan]\n",
      " [ 0.34     -0.078375  0.36     ... -0.33      0.01061        nan]\n",
      " [ 0.37      0.38      0.3      ...  0.46      0.8            nan]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-522eaba18401>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     clusters = run_clique(data=data,\n\u001b[1;32m     48\u001b[0m                           \u001b[0mxsi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxsi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                           tau=tau)\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;31m#save_to_file(clusters, output_file)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m#print(\"\\nClusters exported to \" + output_file)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-af32878450f9>\u001b[0m in \u001b[0;36mrun_clique\u001b[0;34m(data, xsi, tau)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_clique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxsi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# Finding 1 dimensional dense units\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0mdense_units\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_one_dim_dense_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxsi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;31m# Getting 1 dimensional clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-af32878450f9>\u001b[0m in \u001b[0;36mget_one_dim_dense_units\u001b[0;34m(data, tau, xsi)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_one_dim_dense_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxsi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mnumber_of_data_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mnumber_of_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m     \u001b[0mprojection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxsi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_of_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import arffreader as ar\n",
    "import numpy as np\n",
    "# Sample run: python Clique.py mouse.csv [0,1] 2 3 0.3 \" \" output_clusters.txt\n",
    "if __name__ == \"__main__\":\n",
    "    # Clustering with command line parameters\n",
    "    '''if len(sys.argv) > 7:\n",
    "        file_name = sys.argv[1]\n",
    "        feature_columns = literal_eval(sys.argv[2])\n",
    "        label_column = int(sys.argv[3])\n",
    "        xsi = int(sys.argv[4])\n",
    "        tau = float(sys.argv[5])\n",
    "        delimiter = sys.argv[6]\n",
    "        output_file = sys.argv[7]'''\n",
    "    # Sample clustering with default parameters\n",
    "    #else:\n",
    "    #file_name = \"datasets/Archive/diabetes.csv\"\n",
    "    #original_data , labels = ar.readarff(\"datasets/Archive/glass.arff\") #read from arff\n",
    "    #print(original_data)\n",
    "    original_data = np.genfromtxt('/home/munindra/Major_proj/datasets/Archive/B-cell2.csv',delimiter=',',skip_header=1 ) #path to csv file\n",
    "    # change column value here\n",
    "    sup = [item[-1] for item in original_data]\n",
    "    sup = np.array(sup)\n",
    "    labels = sup.ravel()\n",
    "    labels = labels.astype(int)\n",
    "    labels = labels.tolist()\n",
    "    print(original_data)\n",
    "    #feature_columns = [0, 1]\n",
    "    #label_column = 74\n",
    "    xsi = 10\n",
    "    tau = 0.2\n",
    "    delimiter = ','\n",
    "    #output_file = \"synthetic.txt\"\n",
    "\n",
    "    #print(\"Running CLIQUE algorithm on dataset, feature columns = \" +\n",
    "    #      str(feature_columns) + \", label column = \" + str(label_column) + \", xsi = \" +\n",
    "    #      str(xsi) + \", tau = \" + str(tau) + \"\\n\")\n",
    "\n",
    "    # Read in data with labels\n",
    "    #path = os.path.join(os.path.abspath(os.path.dirname(__file__)), file_name)\n",
    "    #original_data = read_data(delimiter, feature_columns, file_name)\n",
    "    #labels = read_labels(delimiter, label_column, path)\n",
    "\n",
    "    # Normalize each dimension to the [0,1] range\n",
    "    data = normalize_features(original_data)\n",
    "    data = data[~np.isnan(data)]\n",
    "\n",
    "    clusters = run_clique(data=data,\n",
    "                          xsi=xsi,\n",
    "                          tau=tau)\n",
    "    #save_to_file(clusters, output_file)\n",
    "    #print(\"\\nClusters exported to \" + output_file)\n",
    "    #print(clusters)\n",
    "    # Evaluate results\n",
    "    f1 , p = evaluate_clustering_performance(original_data, clusters, labels)\n",
    "    print(\"f1_value is :\",np.average(f1))\n",
    "    print(\"purity is :\",np.average(p))\n",
    "\n",
    "    # Visualize clusters\n",
    "    #title = (\"DS: \" + file_name + \" - Params: Tau=\" +\n",
    "    #         str(tau) + \" Xsi=\" + str(xsi))\n",
    "    #if len(feature_columns) <= 2:\n",
    "    #    plot_clusters(data, clusters, title, xsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
