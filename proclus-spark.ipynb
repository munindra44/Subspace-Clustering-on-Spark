{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import arffreader as ar\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "'''\n",
    "def findDimensions1(X, y, k, l, L, Mcurr):\n",
    "\tN, d = y.shape\n",
    "\tDis = [] # dimensions picked for the clusters\n",
    "\n",
    "\tZis = [] # Z for the remaining dimensions\n",
    "\tRem = [] # remaining dimensions\n",
    "\tMselidx = [] # id of the medoid indexing the dimensions in Zis and Rem\n",
    "\n",
    "\tfor i in range(len(Mcurr)):\n",
    "\t\tmi = Mcurr[i]\n",
    "\t\tprint(L[i])# Xij is the average distance from the points in L_i to m_i\n",
    "\t\ta = X.collect()[mi]\n",
    "\t\t#Xij = X.groupByKey().map(lambda row: np.abs(row[L[i]] - row[mi]).sum(axis = 0) / len(L[i]))\n",
    "\t\tXi = np.zeros(len(a))\n",
    "\t\t\n",
    "\t\tfor j in range(len(L[i])):\n",
    "\t\t\tb = L[i][j] \n",
    "\t\t\tc = X.collect()[b]\n",
    "\t\t\tx = np.asarray(c)\n",
    "\t\t\ty = np.asarray(a)\n",
    "\t\t\tXg = np.abs(x - y).sum(axis = 0)\n",
    "\t\t\t#Xi = np.add(Xg, Xi) #print(Xi) \n",
    "\t\tXij = Xg / len(L[i])\n",
    "\t\t#Xij = X.map(lambda X: np.abs(X.features - a)) #print(X[L[i]])# Xij here is an array, containing the avg dists in each dimension\n",
    "\t\t#print(Xij.take(1).foreach(println)) #Xij = np.abs(X[L[i]] - X[mi]).sum(axis = 0) / len(L[i])\n",
    "\t\tprint(\"XIJ IS :\",Xij)#print(Xij)\n",
    "\t\tYi = Xij.sum() / d # average distance over all dimensions\n",
    "\t\tDi = [] # relevant dimensions for m_i\n",
    "\t\tsi = np.sqrt(((Xij - Yi)**2).sum() / (d-1)) # standard deviations\n",
    "\t\tZij = (Xij - Yi) / si # z-scores of distances\n",
    "\t\tprint(Zij)\n",
    "\t\t# pick the smallest two:\n",
    "\t\to = np.argsort(Zij)\n",
    "\t\t#o = o[0]\n",
    "\t\tprint(o)\n",
    "\t\tDi.append(o[0])\n",
    "\t\tDi.append(o[1])\n",
    "\t\tDis.append(Di)\n",
    "\n",
    "\t\tfor j in range(2,d):\n",
    "\t\t\tZis.append(Zij[o[j]])\n",
    "\t\t\tRem.append(o[j])\n",
    "\t\t\tMselidx.append(i)\n",
    "\n",
    "\tif l != 2:\n",
    "\t\t# we need to pick the remaining dimensions\n",
    "\n",
    "\t\to = np.argsort(Zis)\n",
    "\t\t\n",
    "\t\tnremaining = k * l - k * 2\n",
    "\t\t# print \"still need to pick %d dimensions.\" % nremaining\n",
    "\n",
    "\t\t# we pick the remaining dimensions using a greedy strategy:\n",
    "\t\tj = 0\n",
    "\t\twhile nremaining > 0:\n",
    "\t\t\tmidx = Mselidx[o[j]]\n",
    "\t\t\tDis[midx].append(Rem[o[j]])\n",
    "\t\t\tj += 1\n",
    "\t\t\tnremaining -= 1\n",
    "\n",
    "\t#print \"selected:\"\n",
    "\t#print Dis\n",
    "\n",
    "\treturn Dis\n",
    "\t\t\n",
    "'''\n",
    "def manhattanSegmentalDist(x, y, Ds):\n",
    "\t\"\"\" Compute the Manhattan Segmental Distance between x and y considering\n",
    "\t\tthe dimensions on Ds.\"\"\"\n",
    "\tdist = 0\n",
    "\tfor d in Ds:\n",
    "\t\tdist += np.abs(x[d] - y[d])\n",
    "\treturn dist / len(Ds)\n",
    "\n",
    "def assignPoints(X, y, Mcurr, Dis):\n",
    "\n",
    "\tassigns = np.ones(y.shape[0]) * -1\n",
    "\t#X = X\n",
    "\tfor i in range(y.shape[0]):\n",
    "\t\tminDist = np.inf\n",
    "\t\tbest = -1\n",
    "\t\tfor j in range(len(Mcurr)):\n",
    "\t\t\tb = X.collect()[i]\n",
    "\t\t\ta = X.collect()[Mcurr[j]]#dist = X.zipWithIndex().filter(lambda (key,index) : index = i).map(lambda (key,index) : key).collect()\n",
    "\t\t\tdist = manhattanSegmentalDist(b.features, a.features, Dis[j])\n",
    "\t\t\t#dist = dis.first()#dist = dist.astype(np.int32)\n",
    "\t\t\tprint(\"dis is:\",dist,\"itertion\",j,i)\n",
    "\t\t\t#dist = manhattanSegmentalDist(X[i], X[Mcurr[j]], Dis[j])\n",
    "\t\t\tif dist < minDist:\n",
    "\t\t\t\tminDist = dist\n",
    "\t\t\t\tbest = Mcurr[j]\n",
    "\n",
    "\t\tassigns[i] = best\n",
    "\t#print(assigns)\n",
    "\treturn assigns\n",
    "\n",
    "\n",
    "def evaluateClusters(X, assigns, Dis, Mcurr):\n",
    "\n",
    "\tupperSum = 0.0\n",
    "\n",
    "\tfor i in range(len(Mcurr)):\t\t\n",
    "\t\tC = X[np.where(assigns == Mcurr[i])[0]] # points in cluster M_i\n",
    "\t\tCm = C.sum(axis = 0) / C.shape[0] # cluster centroid\n",
    "\t\tYsum = 0.0\n",
    "\n",
    "\t\tfor d in Dis[i]:\n",
    "\t\t\t# avg dist to centroid along dim d:\n",
    "\t\t\tYsum += np.sum(np.abs(C[:,d] - Cm[d])) / C.shape[0]\n",
    "\t\twi = Ysum / len(Dis[i])\n",
    "\n",
    "\t\tupperSum += C.shape[0] * wi\n",
    "\n",
    "\treturn upperSum / X.shape[0]\n",
    "\n",
    "def computeBadMedoids(X, assigns, Dis, Mcurr, minDeviation):\n",
    "\tN, d = X.shape\n",
    "\tk = len(Mcurr)\n",
    "\tMbad = []\n",
    "\tcounts = [len(np.where(assigns == i)[0]) for i in Mcurr]\n",
    "\tcte = int(np.ceil((N / k) * minDeviation))\n",
    "\n",
    "\t# get the medoid with least points:\n",
    "\tMbad.append(Mcurr[np.argsort(counts)[0]])\n",
    "\n",
    "\tfor i in range(len(counts)):\n",
    "\t\tif counts[i] < cte and Mcurr[i] not in Mbad:\n",
    "\t\t\tMbad.append(Mcurr[i])\n",
    "\n",
    "\treturn Mbad\n",
    "\n",
    "def proclus(X, y, k = 2, l = 3, minDeviation = 0.1, A = 30, B = 3, niters = 30, seed = 1234):\n",
    "\t\"\"\" Run PROCLUS on a database to obtain a set of clusters and \n",
    "\t\tdimensions associated with each one.\n",
    "\t\tParameters:\n",
    "\t\t----------\n",
    "\t\t- X: \t   \t\tthe data set\n",
    "\t\t- k: \t   \t\tthe desired number of clusters\n",
    "\t\t- l:\t   \t\taverage number of dimensions per cluster\n",
    "\t\t- minDeviation: for selection of bad medoids\n",
    "\t\t- A: \t   \t\tconstant for initial set of medoids\n",
    "\t\t- B: \t   \t\ta smaller constant than A for the final set of medoids\n",
    "\t\t- niters:  \t\tmaximum number of iterations for the second phase\n",
    "\t\t- seed:    \t\tseed for the RNG\n",
    "\t\"\"\"\n",
    "\tnp.random.seed(seed)\n",
    "\n",
    "\tN, d = y.shape\n",
    "\n",
    "\tif B > A:\n",
    "\t\traise Exception(\"B has to be smaller than A.\")\n",
    "\n",
    "\tif l < 2:\n",
    "\t\traise Exception(\"l must be >=2.\")\n",
    "\n",
    "\t###############################\n",
    "\t# 1.) Initialization phase\n",
    "\t###############################\n",
    "\n",
    "\t# first find a superset of the set of k medoids by random sampling\n",
    "\tidxs = np.arange(N)\n",
    "\tnp.random.shuffle(idxs)\n",
    "\tS = idxs[0:(A*k)]\n",
    "\tM = greedy2(X, S, B * k)\n",
    "\tprint(\"medoids are \",M)\n",
    "\t###############################\n",
    "\t# 2.) Iterative phase\n",
    "\t###############################\n",
    "\n",
    "\tBestObjective = np.inf\n",
    "\n",
    "\t# choose a random set of k medoids from M:\n",
    "\tMcurr = np.random.permutation(M)[0:k] # M current\n",
    "\tMbest = None # Best set of medoids found\n",
    "\n",
    "\tD = squareform(pdist(y)) # precompute the euclidean distance matrix\n",
    "\t#print(D)\n",
    "\tit = 0 # iteration counter\n",
    "\tL = [] # locality sets of the medoids, i.e., points within delta_i of m_i.\n",
    "\tDis = [] # important dimensions for each cluster\n",
    "\tassigns = [] # cluster membership assignments\n",
    "\n",
    "\twhile True:\n",
    "\t\tit += 1\n",
    "\t\tL = []#print(it)\n",
    "\n",
    "\t\tfor i in range(len(Mcurr)):\n",
    "\t\t\tmi = Mcurr[i]\n",
    "\t\t\t# compute delta_i, the distance to the nearest medoid of m_i:\n",
    "\t\t\tdi = D[mi,np.setdiff1d(Mcurr, mi)].min()\n",
    "\t\t\t#print(di)# compute L_i, points in sphere centered at m_i with radius d_i\n",
    "\t\t\tL.append(np.where(D[mi] <= di)[0])\n",
    "\n",
    "\t\t#print(L)# find dimensions:\n",
    "\t\tDis = findDimensions2(X, y, k, l, L, Mcurr)\n",
    "\t\tprint(\"dimensions are:\",Dis)\n",
    "\t\t# form the clusters:\n",
    "\t\tassigns = assignPoints(X, y, Mcurr, Dis)\n",
    "\t\t\n",
    "\t\t# evaluate the clusters:\n",
    "\t\tObjectiveFunction = evaluateClusters(X, assigns, Dis, Mcurr)\n",
    "\t\tprint(ObjectiveFunction)\n",
    "\t\tbadM = [] # bad medoids\n",
    "\n",
    "\t\tMold = Mcurr.copy()\n",
    "\n",
    "\t\tif ObjectiveFunction < BestObjective:\n",
    "\t\t\tBestObjective = ObjectiveFunction\n",
    "\t\t\tMbest = Mcurr.copy()\n",
    "\t\t\t# compute the bad medoids in Mbest:\n",
    "\t\t\tbadM = computeBadMedoids(X, assigns, Dis, Mcurr, minDeviation)\n",
    "\t\t\tprint (\"bad medoids:\")\n",
    "\t\t\tprint (badM)\n",
    "\n",
    "\t\tif len(badM) > 0:\n",
    "\t\t\t# replace the bad medoids with random points from M:\n",
    "\t\t\tprint (\"old mcurr:\")\n",
    "\t\t\tprint (Mcurr)\n",
    "\t\t\tMavail = np.setdiff1d(M, Mbest)\n",
    "\t\t\tnewSel = np.random.choice(Mavail, size = len(badM), replace = False)\n",
    "\t\t\tMcurr = np.setdiff1d(Mbest, badM)\n",
    "\t\t\tMcurr = np.union1d(Mcurr, newSel)\n",
    "\t\t\tprint (\"new mcurr:\")\n",
    "\t\t\tprint (Mcurr)\n",
    "\n",
    "\t\tprint (\"finished iter: %d\" % it)\n",
    "\n",
    "\t\tif np.allclose(Mold, Mcurr) or it >= niters:\n",
    "\t\t\tbreak\n",
    "\n",
    "\tprint (\"finished iterative phase...\")\n",
    "\n",
    "\t###############################\n",
    "\t# 3.) Refinement phase\n",
    "\t###############################\n",
    "\n",
    "\t# compute a new L based on assignments:\n",
    "\tL = []\n",
    "\tfor i in range(len(Mcurr)):\n",
    "\t\tmi = Mcurr[i]\n",
    "\t\tL.append(np.where(assigns == mi)[0])\n",
    "\n",
    "\tDis = findDimensions(X, y, k, l, L, Mcurr)\n",
    "\tassigns = assignPoints(X, Mcurr, Dis)\n",
    "\n",
    "\t# handle outliers:\n",
    "\n",
    "\t# smallest Manhattan segmental distance of m_i to all (k-1)\n",
    "\t# other medoids with respect to D_i:\n",
    "\tdeltais = np.zeros(k)\n",
    "\tfor i in range(k):\n",
    "\t\tminDist = np.inf\n",
    "\t\tfor j in range(k):\n",
    "\t\t\tif j != i:\n",
    "\t\t\t\tdist = manhattanSegmentalDist(X[Mcurr[i]], X[Mcurr[j]], Dis[i])\n",
    "\t\t\t\tif dist < minDist:\n",
    "\t\t\t\t\tminDist = dist\n",
    "\t\tdeltais[i] = minDist\n",
    "\n",
    "\t# mark as outliers the points that are not within delta_i of any m_i:\n",
    "\tfor i in range(len(assigns)):\n",
    "\t\tclustered = False\n",
    "\t\tfor j in range(k):\n",
    "\t\t\td = manhattanSegmentalDist(X[Mcurr[j]], X[i], Dis[j])\n",
    "\t\t\tif d <= deltais[j]:\n",
    "\t\t\t\tclustered = True\n",
    "\t\t\t\tbreak\n",
    "\t\tif not clustered:\n",
    "\t\t\t\n",
    "\t\t\t#print \"marked an outlier\"\n",
    "\t\t\tassigns[i] = -1\n",
    "\n",
    "\treturn (Mcurr, Dis, assigns)\n",
    "\n",
    "def computeBasicAccuracy(pred, expect):\n",
    "\t\"\"\" Computes the clustering accuracy by assigning\n",
    "\t\ta class to each cluster based on majority\n",
    "\t\tvoting and then comparing with the expected\n",
    "\t\tclass. \"\"\"\n",
    "\n",
    "\tif len(pred) != len(expect):\n",
    "\t\traise Exception(\"pred and expect must have the same length.\")\n",
    "\n",
    "\tuclu = np.unique(pred)\n",
    "\n",
    "\tacc = 0.0\n",
    "\n",
    "\tfor cl in uclu:\n",
    "\t\tpoints = np.where(pred == cl)[0]\n",
    "\t\tpclasses = expect[points]\n",
    "\t\tuclass = np.unique(pclasses)\n",
    "\t\tcounts = [len(np.where(pclasses == u)[0]) for u in uclass]\n",
    "\t\tmcl = uclass[np.argmax(counts)]\n",
    "\t\tacc += np.sum(np.repeat(mcl, len(points)) == expect[points])\n",
    "\n",
    "\tacc /= len(pred)\n",
    "\n",
    "\treturn acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(x,y):   \n",
    "    return numpy.sqrt(numpy.sum((x-y)**2))\n",
    "\n",
    "def greedy2(X, S, k):\n",
    "\t# remember that k = B * k here...\n",
    "\n",
    "\tM = [np.random.permutation(S)[0]] # M = {m_1}, a random point in S\n",
    "\t# print M\n",
    "\n",
    "\tA = np.setdiff1d(S, M) # A = S \\ M\n",
    "\t#print(len(A))\n",
    "\tdists = np.zeros(len(A))\n",
    "\n",
    "\t#for i in range(len(A)):\n",
    "\ty = X.collect()[M[0]]\n",
    "\t#print(y.features)#dis = X.filter\n",
    "\tdist = X.filter(lambda x: any(x[9] == A[j] for j in range(len(A))))#.map(lambda x: np.linalg.norm(x.features-y.features))\n",
    "\t#print(dist.count())\n",
    "\tdists = dist.map(lambda x: np.linalg.norm(x.features-y.features)).collect()\n",
    "\tprint(dists)\n",
    "\t\n",
    "\t#dists[i] = np.linalg.norm(X[A[i]] - X[M[0]]) # euclidean distance\n",
    "\n",
    "\t# print dists\n",
    "\n",
    "\tfor i in range(1, k):\n",
    "\t\t# choose medoid m_i as the farthest from previous medoids\n",
    "\n",
    "\t\tmidx = np.argmax(dists)\n",
    "\t\tmi = A[midx]\n",
    "\t\t\n",
    "\t\tM.append(mi)\n",
    "\t\t\t\t\n",
    "\t\t# update the distances, so they reflect the dist to the closest medoid:\n",
    "\t\tfor j in range(len(A)):\n",
    "\t\t\ty = X.collect()[mi]\n",
    "\t\t\tb = X.collect()[A[j]]#filter(lambda x: x[9] == A[j])\n",
    "\t\t\t#print(b.features)#.map(lambda x: np.linalg.norm(X.features - y.features)).collect()\n",
    "\t\t\tdists[j] = min(dists[j], np.linalg.norm(b.features - y.features))\n",
    "\t\t\n",
    "\t\t# remove mi entries from A and dists:\n",
    "\t\tA = np.delete(A, midx)\n",
    "\t\tdists = np.delete(dists, midx)\n",
    "\n",
    "\treturn np.array(M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDimensions2(X, y, k, l, L, Mcurr):\n",
    "\tN, d = y.shape\n",
    "\tDis = [] # dimensions picked for the clusters\n",
    "\t#print(len(L[0]))\n",
    "\tZis = [] # Z for the remaining dimensions\n",
    "\tRem = [] # remaining dimensions\n",
    "\tMselidx = [] # id of the medoid indexing the dimensions in Zis and Rem\n",
    "\n",
    "\tfor i in range(len(Mcurr)):\n",
    "\t\tmi = Mcurr[i]\n",
    "\t\tm = X.collect()[mi]\n",
    "\t\tDavg = X.filter(lambda x: any(x[9] == L[i][j] for j in range(len(L[i]))))# Xij is the average distance from the points in L_i to m_i\n",
    "\t\t#print(Davg.count())\n",
    "\t\tXij = Davg.map(lambda x: np.abs(x.features - m.features)).reduce(lambda x,y: np.true_divide(np.add(x,y),len(L[i])))#.reduce() # Xij here is an array, containing the avg dists in each dimension\n",
    "\t\t#print(Xij)\n",
    "\t\t#Xij = np.abs(X[L[i]] - X[mi]).sum(axis = 0) / len(L[i])\n",
    "\t\tprint(\"XIJ IS :\",Xij)\n",
    "\t\tYi = Xij.sum() / d # average distance over all dimensions\n",
    "\t\tDi = [] # relevant dimensions for m_i\n",
    "\t\tsi = np.sqrt(((Xij - Yi)**2).sum() / (d-1)) # standard deviations\n",
    "\t\tZij = (Xij - Yi) / si # z-scores of distances\n",
    "\n",
    "\t\to = np.argsort(Zij)# pick the smallest two:\n",
    "\t\tprint(o)\n",
    "\t\tDi.append(o[0])\n",
    "\t\tDi.append(o[1])\n",
    "\t\tDis.append(Di)\n",
    "\n",
    "\t\tfor j in range(2,d):\n",
    "\t\t\tZis.append(Zij[o[j]])\n",
    "\t\t\tRem.append(o[j])\n",
    "\t\t\tMselidx.append(i)\n",
    "\n",
    "\tif l != 2:\n",
    "\t\t# we need to pick the remaining dimensions\n",
    "\n",
    "\t\to = np.argsort(Zis)\n",
    "\t\t\n",
    "\t\tnremaining = k * l - k * 2\n",
    "\t\t# print \"still need to pick %d dimensions.\" % nremaining\n",
    "\n",
    "\t\t# we pick the remaining dimensions using a greedy strategy:\n",
    "\t\tj = 0\n",
    "\t\twhile nremaining > 0:\n",
    "\t\t\tmidx = Mselidx[o[j]]\n",
    "\t\t\tDis[midx].append(Rem[o[j]])\n",
    "\t\t\tj += 1\n",
    "\t\t\tnremaining -= 1\n",
    "\n",
    "\t#print \"selected:\"\n",
    "\t#print Dis\n",
    "\n",
    "\treturn Dis\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "file is of type MapPartitionsRDD[12] at javaToPython at NativeMethodAccessorImpl.java:0\n",
      "Using seed 9028844\n",
      "[457.0306528357313, 438.6111741540644, 424.2274126114329, 381.1429979306471, 377.1769444346898, 368.3322316188832, 355.1193037830944, 323.3036621469794, 320.6314855809329, 294.9615217006924, 277.01973617321136, 269.0780862494375, 253.25870239510786, 228.95145453531566, 735.1065271264796, 231.07565919372638, 201.70526440577396, 190.09708446438142, 199.01539691518505, 182.10427853209802, 524.5896156844912, 137.66374484632573, 223.11900615405156, 246.47828843116974, 124.86669147302597, 120.15847878948175, 113.01449921080228, 93.28711701040662, 249.990188370825, 27.422940925515494, 51.55594757907811, 81.00683260691409, 75.57436008447637, 121.98933253887259, 73.66452229605935, 82.94538028719748, 108.17017414612964, 117.05063007942672, 230.2982750098025, 127.77279762922856, 279.29520224527425, 316.45626242280406, 164.08029479327183, 230.88541362609945, 170.1088457808457, 241.2063415772177, 181.16631905669303, 187.25455461751918, 241.28290971418343, 215.4501624721611, 244.54359137879015, 265.4142395652038, 246.70544561116844, 261.52984628551803, 255.6381104138077, 256.98227348748486, 319.4790341117766, 357.48796892680406, 320.98210599607904]\n",
      "medoids are  [441 228   4 748 193 425]\n",
      "XIJ IS : [1.62074385e-02 6.79847894e-02 1.13426013e-01 5.01619099e-02\n",
      " 4.74517897e-07 3.54951300e-02 4.26167370e-04 2.75264613e-02\n",
      " 1.61812722e-03 9.30305315e-01]\n",
      "[4 6 8 0 7 5 3 1 2 9]\n",
      "XIJ IS : [1.96061428e-03 2.14196598e-02 3.89558239e-02 6.80064684e-02\n",
      " 3.26425964e-01 2.53064805e-02 3.77276592e-03 2.71958215e-02\n",
      " 3.77771899e-06 1.48248649e+00]\n",
      "[8 0 6 1 5 7 2 3 4 9]\n",
      "dimensions are: [[4, 6, 8, 0], [8, 0, 6, 1, 5, 7]]\n",
      "dis is: 1.2622499912977219 itertion 0 0\n",
      "dis is: 7.526833355426788 itertion 1 0\n",
      "dis is: 2.806749999523163 itertion 0 1\n",
      "dis is: 12.406166364749273 itertion 1 1\n",
      "dis is: 0.773499995470047 itertion 0 2\n",
      "dis is: 12.735999892155329 itertion 1 2\n",
      "dis is: 26.35275000333786 itertion 0 3\n",
      "dis is: 13.18683303395907 itertion 1 3\n",
      "dis is: 45.177500024437904 itertion 0 4\n",
      "dis is: 0.0 itertion 1 4\n",
      "dis is: 1.844250001013279 itertion 0 5\n",
      "dis is: 8.264499699076017 itertion 1 5\n",
      "dis is: 24.082500003278255 itertion 0 6\n",
      "dis is: 13.856666430830956 itertion 1 6\n",
      "dis is: 0.6110000014305115 itertion 0 7\n",
      "dis is: 7.825666556755702 itertion 1 7\n",
      "dis is: 138.10500000044703 itertion 0 8\n",
      "dis is: 16.121666428943474 itertion 1 8\n",
      "dis is: 0.8365000039339066 itertion 0 9\n",
      "dis is: 14.359333097934723 itertion 1 9\n",
      "dis is: 2.0967500023543835 itertion 0 10\n",
      "dis is: 7.0995000178615255 itertion 1 10\n",
      "dis is: 0.2602500021457672 itertion 0 11\n",
      "dis is: 8.14183309674263 itertion 1 11\n",
      "dis is: 0.7157499939203262 itertion 0 12\n",
      "dis is: 8.974499702453613 itertion 1 12\n",
      "dis is: 214.04500000178814 itertion 0 13\n",
      "dis is: 15.648333032925924 itertion 1 13\n",
      "dis is: 45.252250000834465 itertion 0 14\n",
      "dis is: 11.83349988857905 itertion 1 14\n",
      "dis is: 1.0235000029206276 itertion 0 15\n",
      "dis is: 9.98399976392587 itertion 1 15\n",
      "dis is: 60.256750002503395 itertion 0 16\n",
      "dis is: 4.239500145117442 itertion 1 16\n",
      "dis is: 1.0810000002384186 itertion 0 17\n",
      "dis is: 9.088999698559443 itertion 1 17\n",
      "dis is: 23.59875000268221 itertion 0 18\n",
      "dis is: 6.3841668119033175 itertion 1 18\n",
      "dis is: 26.512250006198883 itertion 0 19\n",
      "dis is: 5.709833353757858 itertion 1 19\n",
      "dis is: 61.03149999678135 itertion 0 20\n",
      "dis is: 4.397333224614461 itertion 1 20\n",
      "dis is: 1.0474999994039536 itertion 0 21\n",
      "dis is: 12.266666173934937 itertion 1 21\n",
      "dis is: 1.031750001013279 itertion 0 22\n",
      "dis is: 13.18949988981088 itertion 1 22\n",
      "dis is: 0.5787499994039536 itertion 0 23\n",
      "dis is: 7.854166428248088 itertion 1 23\n",
      "dis is: 36.58100000023842 itertion 0 24\n",
      "dis is: 7.255666683117549 itertion 1 24\n",
      "dis is: 29.09325000271201 itertion 0 25\n",
      "dis is: 7.347166366875172 itertion 1 25\n",
      "dis is: 1.0802500024437904 itertion 0 26\n",
      "dis is: 5.4551661759614944 itertion 1 26\n",
      "dis is: 37.772750005126 itertion 0 27\n",
      "dis is: 12.45016630490621 itertion 1 27\n",
      "dis is: 28.333250001072884 itertion 0 28\n",
      "dis is: 11.490499635537466 itertion 1 28\n",
      "dis is: 1.810249999165535 itertion 0 29\n",
      "dis is: 6.991833349068959 itertion 1 29\n",
      "dis is: 1.7580000013113022 itertion 0 30\n",
      "dis is: 11.64033309618632 itertion 1 30\n",
      "dis is: 63.31825000047684 itertion 0 31\n",
      "dis is: 6.989499698082606 itertion 1 31\n",
      "dis is: 15.827750004827976 itertion 0 32\n",
      "dis is: 14.053499892354012 itertion 1 32\n",
      "dis is: 1.597500003874302 itertion 0 33\n",
      "dis is: 13.716666494806608 itertion 1 33\n",
      "dis is: 0.5164999961853027 itertion 0 34\n",
      "dis is: 9.212666362524033 itertion 1 34\n",
      "dis is: 50.09700000286102 itertion 0 35\n",
      "dis is: 9.903666426738104 itertion 1 35\n",
      "dis is: 0.28950000554323196 itertion 0 36\n",
      "dis is: 4.461332971851031 itertion 1 36\n",
      "dis is: 0.5217500030994415 itertion 0 37\n",
      "dis is: 11.47049950559934 itertion 1 37\n",
      "dis is: 2.2687499970197678 itertion 0 38\n",
      "dis is: 10.280832966168722 itertion 1 38\n",
      "dis is: 53.702999994158745 itertion 0 39\n",
      "dis is: 9.983000020186106 itertion 1 39\n",
      "dis is: 19.82675000280142 itertion 0 40\n",
      "dis is: 10.852833097179731 itertion 1 40\n",
      "dis is: 1.2794999927282333 itertion 0 41\n",
      "dis is: 3.4153329730033875 itertion 1 41\n",
      "dis is: 1.3357500024139881 itertion 0 42\n",
      "dis is: 12.742166303098202 itertion 1 42\n",
      "dis is: 60.53575000166893 itertion 0 43\n",
      "dis is: 11.311167190472284 itertion 1 43\n",
      "dis is: 1.3210000023245811 itertion 0 44\n",
      "dis is: 9.11566649377346 itertion 1 44\n",
      "dis is: 3.0787499994039536 itertion 0 45\n",
      "dis is: 8.749166429042816 itertion 1 45\n",
      "dis is: 2.753499999642372 itertion 0 46\n",
      "dis is: 5.020666301250458 itertion 1 46\n",
      "dis is: 2.502000004053116 itertion 0 47\n",
      "dis is: 16.133666425943375 itertion 1 47\n",
      "dis is: 1.058499999344349 itertion 0 48\n",
      "dis is: 8.15733334918817 itertion 1 48\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Indices must be of type integer, got type <class 'numpy.int64'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-2af7b8bddd17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproclus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m#print \"Accuracy: %.4f\" % computeBasicAccuracy(A, sup)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-13843d7d68c2>\u001b[0m in \u001b[0;36mproclus\u001b[0;34m(X, y, k, l, minDeviation, A, B, niters, seed)\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dimensions are:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0;31m# form the clusters:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                 \u001b[0massigns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massignPoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMcurr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0;31m# evaluate the clusters:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-13843d7d68c2>\u001b[0m in \u001b[0;36massignPoints\u001b[0;34m(X, y, Mcurr, Dis)\u001b[0m\n\u001b[1;32m     94\u001b[0m                         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMcurr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#dist = X.zipWithIndex().filter(lambda (key,index) : index = i).map(lambda (key,index) : key).collect()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanhattanSegmentalDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                         \u001b[0;31m#dist = dis.first()#dist = dist.astype(np.int32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dis is:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"itertion\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-13843d7d68c2>\u001b[0m in \u001b[0;36mmanhattanSegmentalDist\u001b[0;34m(x, y, Ds)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mdist\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/linalg/__init__.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             raise TypeError(\n\u001b[0;32m--> 716\u001b[0;31m                 \"Indices must be of type integer, got type %s\" % type(index))\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Indices must be of type integer, got type <class 'numpy.int64'>"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "#import adjrand\n",
    "#import arffreader as ar\n",
    "\n",
    "##### read data#####\n",
    "\"\"\"\n",
    "#X = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0]).mapPartitions(readPointBatch).cache()\n",
    "#y, sup = ar.readarff(\"/usr/lib/spark/examples/src/main/python/Major_proj/datasets/Archive/leukemia2.csv\") #read from arff\n",
    "#y = genfromtxt('/usr/lib/spark/examples/src/main/python/Major_proj/datasets/Archive/leukemia1.csv',delimiter=',',skip_header=1) #path to csv file\n",
    "y = pd.read_csv('/usr/lib/spark/examples/src/main/python/Major_proj/datasets/Archive/leukemia1.csv',delimiter=',')\n",
    "# change label values here\n",
    "#print(y)\n",
    "print(y.dtypes)\n",
    "#df =  df.withColumn(\"cls\", df[\"CLASS\"].getItem(0).cast(\"string\")) \n",
    "y = y.replace({'ALL': 0, 'AML': 1})\n",
    "sup = [item[-1] for item in y]\n",
    "sup = np.array(sup)\n",
    "sup = sup.ravel()\n",
    "print(y)\n",
    "\"\"\"\n",
    "g = pd.read_csv('/usr/lib/spark/examples/src/main/python/Major_proj/datasets/Archive/diabetes.csv',delimiter=',')\n",
    "#print(y)\n",
    "y = g.iloc[:, :-1]\n",
    "#dftt = dft.toDF()\n",
    "#print(type(dftt))\n",
    "X = dft.rdd\n",
    "print(X.count())\n",
    "print(\"file is of type\",X)\n",
    "Dims = [0,1]\n",
    "#plotter.plotDataset(X, D = Dims) # plot 0-1 dimensions\n",
    "\n",
    "R = 1 # toggle run proclus\n",
    "RS = 0 # toggle use random seed\n",
    "\n",
    "if R: # run proclus\n",
    "\trseed = 9028844\n",
    "\tif RS:\n",
    "\t\trseed = np.random.randint(low = 0, high = 1239831)\n",
    "\n",
    "\tprint (\"Using seed %d\" % rseed)\n",
    "\tk = 2\n",
    "\tl = 5\n",
    "\tM, D, A = proclus(X, y, k, l, seed = rseed)\n",
    "\tprint(A)\n",
    "\t#print \"Accuracy: %.4f\" % computeBasicAccuracy(A, sup)\n",
    "\t#print \"Adjusted rand index: %.4f\" % adjrand.computeAdjustedRandIndex(A, sup)\n",
    "\n",
    "\t#plotter.plotClustering(X, M, A, D = Dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readPointBatch(iterator):\n",
    "    strs = list(iterator)\n",
    "    matrix = np.zeros((len(strs), D + 1))\n",
    "    for i, s in enumerate(strs):\n",
    "        matrix[i] = np.fromstring(s.replace(',', ' '), dtype=np.float32, sep=' ')\n",
    "    return [matrix]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:21: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('id', 'string'),\n",
       " ('Pregnancies', 'string'),\n",
       " ('Glucose', 'string'),\n",
       " ('BloodPressure', 'string'),\n",
       " ('SkinThickness', 'string'),\n",
       " ('Insulin', 'string'),\n",
       " ('BMI', 'string'),\n",
       " ('DiabetesPedigreeFunction', 'string'),\n",
       " ('Age', 'string'),\n",
       " ('Outcome', 'string')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocessing\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "spark = SparkSession.builder.appName(\"proclus\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "#Load and parse the data\n",
    "#dataset = sqlContext.read.format('com.databricks.spark.csv').options(header='true').load('/usr/lib/spark/examples/src/main/python/Major_proj/datasets/Archive/glass.csv')\n",
    "#df = pd.read_csv('/usr/lib/spark/examples/src/main/python/Major_proj/datasets/Archive/glass.csv',delimiter=',') # path to file here\n",
    "dataset = sqlContext.read.format('com.databricks.spark.csv').options(header='true').load('/usr/lib/spark/examples/src/main/python/Major_proj/diabetes.csv')\n",
    "df = pd.read_csv('/usr/lib/spark/examples/src/main/python/Major_proj/diabetes.csv',delimiter=',') # path to file here\n",
    "\n",
    "a = len(df.columns)\n",
    "#print(a)\n",
    "label = df.as_matrix(columns=df.columns[a-1:])\n",
    "label = label.ravel()\n",
    "dataset = dataset.na.drop()\n",
    "#print(label)\n",
    "dataset.dtypes\n",
    "'''\n",
    "from pyspark.sql.types import FloatType\n",
    "dataset = dataset.withColumn(\"id\", dataset[\"id\"].cast(FloatType()))\n",
    "dataset = dataset.withColumn(\"Pregnancies\", dataset[\"Pregnancies\"].cast(FloatType()))\n",
    "dataset = dataset.withColumn(\"Glucose\", dataset[\"Glucose\"].cast(FloatType()))\n",
    "dataset = dataset.withColumn(\"BloodPressure\", dataset[\"BloodPressure\"].cast(FloatType()))\n",
    "dataset = dataset.withColumn(\"SkinThickness\", dataset[\"SkinThickness\"].cast(FloatType()))\n",
    "dataset = dataset.withColumn(\"Insulin\", dataset[\"Insulin\"].cast(FloatType()))\n",
    "dataset = dataset.withColumn(\"DiabetesPedigreeFunction\", dataset[\"DiabetesPedigreeFunction\"].cast(FloatType()))\n",
    "dataset = dataset.withColumn(\"BMI\", dataset[\"BMI\"].cast(FloatType()))\n",
    "dataset = dataset.withColumn(\"Age\", dataset[\"Age\"].cast(FloatType()))\n",
    "'''\n",
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Pregnancies: float, Glucose: float, BloodPressure: float, SkinThickness: float, Insulin: float, BMI: float, DiabetesPedigreeFunction: float, Age: float, Outcome: float, id: bigint, features: vector]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocessing\n",
    "dft = dataset.select(*(dataset[c].cast(\"float\").alias(c) for c in dataset.columns[1:]))\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "\n",
    "dft = dft.select(\"*\").withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "#split outcome\n",
    "colm = (\"'class'\")\n",
    "dft = dft.drop(colm)\n",
    "#print(dft.head())\n",
    "from pyspark.ml.feature import VectorAssembler # in spark api we have to convert data to vectors in order to run the model.\n",
    "\n",
    "# creating an instance of the vector assembler \n",
    "assembler = VectorAssembler(inputCols = dft.columns, outputCol = 'features')\n",
    "\n",
    "# transforming dataframe into vector assembler \n",
    "final_df = assembler.transform(dft)\n",
    "#final_df.drop('Outcome')\n",
    "#final_df = final_df.rdd.map(tuple)\n",
    "#df = final_df.ix[:,-1]\n",
    "#df = final_df.select(\"features\").rdd.map(lambda x: x).collect()\n",
    "#print(df.show())\n",
    "#df = final_df.select('features')\n",
    "#df = dfth[0].features.toArray()     # to array\n",
    "#df = df.rdd\n",
    "#dft = final_df.select(\"features\") \n",
    "dft = final_df\n",
    "dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy(X, S, k):\n",
    "\t# remember that k = B * k here...\n",
    "\n",
    "\tM = [np.random.permutation(S)[0]] # M = {m_1}, a random point in S\n",
    "\t# print M\n",
    "\n",
    "\tA = np.setdiff1d(S, M) # A = S \\ M\n",
    "\tdists = np.zeros(len(A))\n",
    "\ta = M[0]\n",
    "\tprint(X)#values = X.first().collect()\n",
    "\tmedi = X.collect()[a]\n",
    "\t#print(medi)\n",
    "\tfor i in range(len(A)):\n",
    "\t\tb = A[i]\n",
    "\t\titr = X.collect()[b]  \n",
    "\t\t#print(itr)\n",
    "\t\tx = np.asarray(medi)\n",
    "\t\ty = np.asarray(itr)\n",
    "\t\tdists[i] = np.linalg.norm(x[0] - y[0]) # euclidean distance\n",
    "\n",
    "\tprint (dists)\n",
    "\n",
    "\tfor i in range(1, k):\n",
    "\t\t# choose medoid m_i as the farthest from previous medoids\n",
    "\n",
    "\t\tmidx = np.argmax(dists)\n",
    "\t\tmi = A[midx]\n",
    "\t\t\n",
    "\t\tM.append(mi)\n",
    "\t\tmid = X.collect()[mi]\n",
    "\t\t# update the distances, so they reflect the dist to the closest medoid:\n",
    "\t\tfor j in range(len(A)):\n",
    "\t\t\tb = A[j]\n",
    "\t\t\titr_dist = X.collect()[b] \n",
    "\t\t\tx = np.asarray(mid)\n",
    "\t\t\ty = np.asarray(itr_dist)\n",
    "\t\t\tdists[j] = min(dists[j], np.linalg.norm(x[0] - y[0]))\n",
    "\t\t\n",
    "\t\t# remove mi entries from A and dists:\n",
    "\t\tA = np.delete(A, midx)\n",
    "\t\tdists = np.delete(dists, midx)\n",
    "\tprint(np.array(M))\n",
    "\treturn np.array(M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def findDimensions(X, y, k, l, L, Mcurr):\n",
    "\tN, d = y.shape\n",
    "\tDis = [] # dimensions picked for the clusters\n",
    "\n",
    "\tZis = [] # Z for the remaining dimensions\n",
    "\tRem = [] # remaining dimensions\n",
    "\tMselidx = [] # id of the medoid indexing the dimensions in Zis and Rem\n",
    "\n",
    "\tfor i in range(len(Mcurr)):\n",
    "\t\tmi = Mcurr[i]\n",
    "\t\t#a = X.collect()[mi]\n",
    "\t\t# Xij is the average distance from the points in L_i to m_i\n",
    "\t\t#X1 = X.zipWithIndex()\n",
    "\t\t#print(X1.first())\n",
    "\t\tXij = X.filter(lambda X : X[0]).map(absolute(X,L,mi)).collect()  # dimension\n",
    "\t\t#Xij = X.rdd.zipWithIndex().filter(lambda (key,index) : index == L[i]).map(lambda (key,index): np.abs(X[L[i]] - X[mi])/len(L[i])).collect()  # dimension\n",
    "\t\t#Xij = np.abs(X[L[i]] - X[mi]).sum(axis = 0) / len(L[i])\n",
    "\t\tprint(\"XIJ IS :\",Xij)\n",
    "\t\tYi = Xij.sum() / d # average distance over all dimensions\n",
    "\t\tDi = [] # relevant dimensions for m_i\n",
    "\t\tsi = np.sqrt(((Xij - Yi)**2).sum() / (d-1)) # standard deviations\n",
    "\t\tZij = (Xij - Yi) / si # z-scores of distances\n",
    "\n",
    "\t\to = np.argsort(Zij)# pick the smallest two:\n",
    "\t\tprint(o)\n",
    "\t\tDi.append(o[0])\n",
    "\t\tDi.append(o[1])\n",
    "\t\tDis.append(Di)\n",
    "\n",
    "\t\tfor j in range(2,d):\n",
    "\t\t\tZis.append(Zij[o[j]])\n",
    "\t\t\tRem.append(o[j])\n",
    "\t\t\tMselidx.append(i)\n",
    "\n",
    "\tif l != 2:\n",
    "\t\t# we need to pick the remaining dimensions\n",
    "\n",
    "\t\to = np.argsort(Zis)\n",
    "\t\t\n",
    "\t\tnremaining = k * l - k * 2\n",
    "\t\t# print \"still need to pick %d dimensions.\" % nremaining\n",
    "\n",
    "\t\t# we pick the remaining dimensions using a greedy strategy:\n",
    "\t\tj = 0\n",
    "\t\twhile nremaining > 0:\n",
    "\t\t\tmidx = Mselidx[o[j]]\n",
    "\t\t\tDis[midx].append(Rem[o[j]])\n",
    "\t\t\tj += 1\n",
    "\t\t\tnremaining -= 1\n",
    "\n",
    "\t#print \"selected:\"\n",
    "\t#print Dis\n",
    "\n",
    "\treturn Dis\n",
    "\t\t\n",
    "\n",
    "def absolute(X,L,mi):\n",
    "    g = []\n",
    "    for j in range(len(L)):\n",
    "        print(X.collect()[j])\n",
    "        g = np.abs(np.asarray(X.collect()[j]) - np.asarray(X.collect()[mi])).sum(axis = 0)/len(L)\n",
    "        print(g)\n",
    "        y.append(g)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 4, localhost, executor driver): java.io.IOException: Cannot run program \"usr/bin/python3.5\": error=2, No such file or directory\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:197)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:122)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:95)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:108)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 16 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: Cannot run program \"usr/bin/python3.5\": error=2, No such file or directory\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:197)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:122)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:95)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:108)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 16 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-aa3c7c3e9bf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         \"\"\"\n\u001b[0;32m-> 1378\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 4, localhost, executor driver): java.io.IOException: Cannot run program \"usr/bin/python3.5\": error=2, No such file or directory\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:197)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:122)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:95)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:108)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 16 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: Cannot run program \"usr/bin/python3.5\": error=2, No such file or directory\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:197)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:122)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:95)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:108)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 16 more\n"
     ]
    }
   ],
   "source": [
    "dat = dataset.rdd\n",
    "dat.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dat.filter(lambda x: x[0] == 54 or x[0] == 4)\n",
    "y.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
